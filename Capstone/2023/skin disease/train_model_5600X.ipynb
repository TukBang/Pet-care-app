{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA TYPE: train\n",
      "\n",
      "original data folder\n",
      "['A1', 'A2', 'A3', 'A4', 'A5', 'A6']\n",
      "\n",
      "RoI data folder\n",
      "['A1', 'A2', 'A3', 'A4', 'A5', 'A6']\n",
      "\n",
      "box data folder\n",
      "there is no directory.\n",
      "crop data folder\n",
      "there is no directory.\n",
      "ratio_zero-padding data folder\n",
      "there is no directory.\n",
      "RZ_LANCZOS4 data folder\n",
      "['A1', 'A2', 'A3', 'A4', 'A5', 'A6']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# default\n",
    "from pprint import pprint\n",
    "import natsort\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "# data structure\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# visualize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models import efficientnet_v2_s, efficientnet_v2_m, efficientnet_v2_l\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import module.skin_disease_model as sdm\n",
    "from module.data_processing import mkadir\n",
    "\n",
    "TRAIN = 0\n",
    "VALID = 1\n",
    "data_type = [\"train\", \"valid\"]\n",
    "dt = data_type[TRAIN]\n",
    "\n",
    "project_path = \"D:/Capstone/\"\n",
    "original_data_path = os.path.join(project_path, \"dataset/\", \"original/\", dt)\n",
    "RoI_data_path = os.path.join(project_path, \"dataset/\", \"RoI/\", dt)\n",
    "box_data_path = os.path.join(project_path, \"dataset/\", \"box/256x256/\", dt)\n",
    "crop_data_path = os.path.join(project_path, \"dataset/\", \"crop/\", dt)\n",
    "RZ_data_path = os.path.join(project_path, \"dataset/\", \"ratio_zero-padding/\", dt)\n",
    "RZ_LANCZOS4_data_path = os.path.join(project_path, \"dataset/\", \"RZ_LANCZOS4/\", dt)\n",
    "\n",
    "data_path_namelist = [\"original\", \"RoI\", \"box\", \"crop\", \"ratio_zero-padding\", \"RZ_LANCZOS4\"]\n",
    "data_path_list = [original_data_path, RoI_data_path, box_data_path, crop_data_path, RZ_data_path, RZ_LANCZOS4_data_path]\n",
    "\n",
    "print(\"DATA TYPE:\", dt, end=\"\\n\\n\")\n",
    "for i, data_path in enumerate(data_path_list):\n",
    "    print(data_path_namelist[i] + \" data folder\")\n",
    "    try:\n",
    "        folder = natsort.natsorted(os.listdir(data_path))\n",
    "        print(folder, end=\"\\n\\n\")\n",
    "    except:\n",
    "        print(\"there is no directory.\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(x, y, seed):\n",
    "    if len(x) != len(y):\n",
    "        print(\"x and y are not same the length.\")\n",
    "        return\n",
    "\n",
    "    order = np.arange(len(y))\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(order)\n",
    "    \n",
    "    new_x = list()\n",
    "    for i in order:\n",
    "        new_x.append(x[i])\n",
    "    \n",
    "    return new_x, y[order], order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x length:  52914\n",
      "train_x type:  <class 'list'>\n",
      "train_y length:  (52914,)\n",
      "train_y type:  <class 'numpy.ndarray'>\n",
      "valid_x length:  6645\n",
      "valid_x type:  <class 'list'>\n",
      "valid_y length:  (6645,)\n",
      "valid_y type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "size = 224\n",
    "dataset_path = \"/\".join(RZ_LANCZOS4_data_path.split(\"/\")[:-1]) + \"/\"\n",
    "\n",
    "# 이미지 불러오기\n",
    "with open(os.path.join(dataset_path, \"224x224_train_data.pkl\"), \"rb\") as pkl_file:\n",
    "  train_x, train_y = pkl.load(pkl_file)\n",
    "\n",
    "with open(os.path.join(dataset_path, \"224x224_valid_data.pkl\"), \"rb\") as pkl_file:\n",
    "  valid_x, valid_y = pkl.load(pkl_file)\n",
    "\n",
    "print(\"train_x length: \", len(train_x))\n",
    "print(\"train_x type: \", type(train_x))\n",
    "print(\"train_y length: \", train_y.shape)\n",
    "print(\"train_y type: \", type(train_y))\n",
    "print(\"valid_x length: \", len(valid_x))\n",
    "print(\"valid_x type: \", type(valid_x))\n",
    "print(\"valid_y length: \", valid_y.shape)\n",
    "print(\"valid_y type: \", type(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y를 통해서 입력된 숫자대로 클래스를 나누어 리스트로 반환한다.\n",
    "def split_label(x, y, label_num):\n",
    "    u, c = np.unique(y, return_counts=True)\n",
    "\n",
    "    labels = list()\n",
    "    \n",
    "    prev_loc = 0\n",
    "    end_loc = 0\n",
    "    for i, loc in enumerate(c):\n",
    "        end_loc += loc\n",
    "\n",
    "        if (i + 1) % label_num == 0:\n",
    "            labels.append((x[prev_loc:end_loc].copy(), y[prev_loc:end_loc].copy()))\n",
    "            prev_loc = end_loc\n",
    "\n",
    "    if prev_loc != end_loc:\n",
    "        labels.append((x[prev_loc:end_loc].copy(), y[prev_loc:end_loc].copy()))\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sltrain = split_label(train_x, train_y, 3)\n",
    "slvalid = split_label(valid_x, valid_y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5] [8709 8369 6105]\n",
      "[3 4 5] [1092 1056  762]\n"
     ]
    }
   ],
   "source": [
    "DATA = 0\n",
    "LABEL = 1\n",
    "ORDER = 1\n",
    "\n",
    "split_train_x = sltrain[ORDER][DATA]\n",
    "split_train_y = sltrain[ORDER][LABEL]\n",
    "split_valid_x = slvalid[ORDER][DATA]\n",
    "split_valid_y = slvalid[ORDER][LABEL]\n",
    "\n",
    "tu, tc = np.unique(split_train_y, return_counts=True)\n",
    "vu, vc = np.unique(split_valid_y, return_counts=True)\n",
    "\n",
    "print(tu, tc)\n",
    "print(vu, vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_y[split_train_y == 3] = 0\n",
    "split_train_y[split_train_y == 4] = 1\n",
    "split_train_y[split_train_y == 5] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_valid_y[split_valid_y == 3] = 0\n",
    "split_valid_y[split_valid_y == 4] = 1\n",
    "split_valid_y[split_valid_y == 5] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(split_train_y))\n",
    "print(np.unique(split_valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x, valid_x, train_y, valid_y\n",
    "del sltrain, slvalid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = sdm.Skin_Disease_Dataset(train_x, train_y, train_transforms)\n",
    "valid_dataset = sdm.Skin_Disease_Dataset(valid_x, valid_y, test_transforms)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = sdm.Skin_Disease_Dataset(split_train_x, split_train_y, train_transforms)\n",
    "valid_dataset = sdm.Skin_Disease_Dataset(split_valid_x, split_valid_y, test_transforms)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "Layer (type:depth-idx)                                       Output Shape              Param #\n",
      "==============================================================================================================\n",
      "Skin_Distinction_Model                                       [1, 3]                    --\n",
      "├─EfficientNet: 1-1                                          [1, 1000]                 --\n",
      "│    └─Sequential: 2-1                                       [1, 1280, 7, 7]           --\n",
      "│    │    └─Conv2dNormActivation: 3-1                        [1, 24, 112, 112]         696\n",
      "│    │    └─Sequential: 3-2                                  [1, 24, 112, 112]         10,464\n",
      "│    │    └─Sequential: 3-3                                  [1, 48, 56, 56]           303,552\n",
      "│    │    └─Sequential: 3-4                                  [1, 64, 28, 28]           589,184\n",
      "│    │    └─Sequential: 3-5                                  [1, 128, 14, 14]          917,680\n",
      "│    │    └─Sequential: 3-6                                  [1, 160, 14, 14]          3,463,840\n",
      "│    │    └─Sequential: 3-7                                  [1, 256, 7, 7]            14,561,832\n",
      "│    │    └─Conv2dNormActivation: 3-8                        [1, 1280, 7, 7]           330,240\n",
      "│    └─AdaptiveAvgPool2d: 2-2                                [1, 1280, 1, 1]           --\n",
      "│    └─Sequential: 2-3                                       [1, 1000]                 --\n",
      "│    │    └─Dropout: 3-9                                     [1, 1280]                 --\n",
      "│    │    └─Linear: 3-10                                     [1, 1000]                 1,281,000\n",
      "├─Linear: 1-2                                                [1, 3]                    3,003\n",
      "==============================================================================================================\n",
      "Total params: 21,461,491\n",
      "Trainable params: 21,461,491\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 2.85\n",
      "==============================================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 194.82\n",
      "Params size (MB): 85.85\n",
      "Estimated Total Size (MB): 281.27\n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(project_path, \"model/\", \"2023-03-22/\")\n",
    "mkadir(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = sdm.Skin_Distinction_Model(model=efficientnet_v2_s(weights=\"DEFAULT\"),\n",
    "                                   out_features=3,\n",
    "                                   device=device,\n",
    "                                   save_path=model_path).to(device)\n",
    "\n",
    "pprint(summary(model, input_size=(1, 3, 224, 224), verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.000213388\n",
    "# optimizer = optim.SGD(model.parameters(), lr=7.5e-7, momentum=0.999, weight_decay=1e-5, nesterov=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "parameters = {\n",
    "    'Train_DataLoader' : train_data_loader,\n",
    "    'Valid_DataLoader' : valid_data_loader,\n",
    "    'Optimizer'        : optimizer,\n",
    "    'Loss_function'    : criterion,\n",
    "    'Epochs'           : epochs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a113e9756cf2460db874fdf73158b6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_x, split_train_y, Torder = shuffle_data(split_train_x, split_train_y, 42)\n",
    "split_valid_x, split_valid_y, Vorder = shuffle_data(split_valid_x, split_valid_y, 42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
